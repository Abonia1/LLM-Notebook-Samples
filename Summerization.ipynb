{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summerization - Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abonia.sojasingarayaribm.com/miniconda3/envs/myenv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' In this post, Lilian Weng explores the concept of building autonomous agents using large language models (LLMs). She discusses the importance of clear communication and the role of core classes, functions, methods, and markdown formatting in LLM-powered agent development. The post also covers references to related research papers and projects on this topic.\\n\\nKEYWORDS: nlp, language-model, agent, steerability, prompting.\\n\\nADVERSARIAL ATTACKS ON LLMs:\\nThis section is not present in the given text.\\n\\nPROMPT ENGINEERING:\\nThe post focuses on building autonomous agents using large language models and clear communication but does not specifically discuss prompt engineering. Prompt engineering refers to designing effective prompts for large language models to generate desired outputs or behaviors. However, the concept of clear communication is an essential aspect of prompt engineering.\\n\\nADDITIONAL NOTES:\\nThe text includes references to various research papers, projects, and resources related to LLMs and agent development. These resources can provide valuable insights for those interested in exploring this topic further. Additionally, the post highlights the importance of clear communication, prompting, and steerability in developing agents powered by large language models.\\n\\nREFERENCES:\\nThe given text includes references to the following papers and resources:\\n\\n1. Wei et al., \"Chain of thought prompting elicits reasoning in large language models.\" NeurIPS 2022.\\n2. Yao et al., \"Tree of Thoughts: Dliberate Problem Solving with Large Language Models.\" arXiv preprint arXiv:2305.10601 (2023).\\n3. Liu et al., \"Chain of Hindsight Aligns Language Models with Feedback.\" arXiv preprint arXiv:2302.02676 (2023).\\n4. Liu et al., \"LLM+P: Empowering Large Language Models with Optimal Planning Proficiency\" arXiv preprint arXiv:2304.11477 (2023).\\n5. Yao et al., \"ReAct: Synergizing reasoning and acting in language models.\" ICLR 2023.\\n6. Google Blog, \"Announcing ScaNN: Efficient Vector Similarity Search\" July 28, 2020.\\n7. Shinn & Labash, \"Reflexion: an autonomous agent with dynamic memory and self-reflection.\" arXiv preprint arXiv:2303.11366 (2023).\\n8. Laskin et al., \"In-context Reinforcement Learning with Algorithm Distillation\" ICLR 2023.\\n9. Karpas et al., \"MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.\" arXiv preprint arXiv:2205.00445 (2022).\\n10. Weaviate Blog, \"Why is Vector Search so fast?\" Sep 13, 2022.\\n11. Li et al., \"API-Bank: A Benchmark for Tool-Augmented LLMs\" arXiv preprint arXiv:2304.08244 (2023).\\n12. Shen et al., \"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace\" arXiv preprint arXiv:2303.17580 (2023).\\n13. Bran et al., \"ChemCrow: Augmenting large-language models with chemistry tools.\" arXiv preprint arXiv:2304.05376 (2023).\\n14. Boiko et al., \"Emergent autonomous scientific research capabilities of large language models.\" arXiv preprint arXiv:2304.05332 (2023).\\n15. Joon Sung Park, et al., \"Generative Agents: Interactive Simulacra of Human Behavior.\" arXiv preprint arXiv:2304.03442 (2023).\\n16. AutoGPT, https://github.com/Significant-Gravitas/Auto-GPT\\n17. GPT-Engineer, https://github.com/AntonOsika/gpt-engineer.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "llm = ChatOllama(model=\"mistral\", max_tokens=100) # temperature=0.8\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9417315398884a7e91c8eed862e4eed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee0f291610ea4e529f88564daecd5ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951ec7f8565344b3bdf47ea33c02a4f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c830359b78964a7c87df6b855a00ad31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3062a63409314da7ac22e129c9b56645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1367 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" This post outlines the process of creating an autonomous agent using Python and large language models (LLMs) with Hugging Face's Transformers library. The agent architecture consists of a model, view, and controller. The model is the LLM that generates text based on prompts, while the view displays output to the user, and the controller handles user input to steer the agent towards specific tasks. Techniques like prompt engineering (refining prompts, chain-of-thought, in-context learning) and steerability (adversarial prompting, active learning, transfer learning) can be employed to improve agent performance. The post includes code examples and references to related projects and research.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"mistral\", max_tokens=100) # temperature=0.8\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "\n",
    "chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' In this post, we delve into creating LLM-powered autonomous agents using Python. The key components include the entrypoint file, model class, view class, and controller class. We follow the LLM design philosophy of having a clear separation of concerns and ensuring code is modular and maintainable.\\n\\nThe entrypoint file serves as the main driver, instantiating and running the agent. The model class interacts with the large language model (LLM) for problem solving using various prompting techniques like Chain of Thought, Tree of Thoughts, and Hindsight.\\n\\nThe view class manages user interaction and visualization. Meanwhile, the controller class handles user input, integrates external knowledge sources, and applies reasoning and planning using algorithms like MRKL, In-context RL, and ScaNN search. The agent is steerable through a command line interface or an API.\\n\\nWe also discuss adversarial attacks on LLMs, focusing on their impact and potential mitigation strategies. Additionally, we reference various related research papers that enhance the capabilities of LLMs in problem solving and autonomous agent development.\\n\\nnlp language-model agent steerability prompting prompt-engineering adversarial-attacks large-language-models autonomy problem-solving model-design view-controller research-papers mitigation-strategies\\n\\n\"\\n\\n\\nAdversarial attacks on Large Language Models (LLMs) are becoming a significant concern in the field of natural language processing and artificial intelligence. These attacks can manipulate or mislead LLMs, leading to undesirable outcomes. In this post, we\\'ll discuss some adversarial attack techniques on LLMs and potential strategies for mitigation.\\n\\nAdversarial Attacks on LLMs: Techniques\\n\\n1. **Input Adversarial Attacks**: These attacks focus on crafting malicious inputs to manipulate the model\\'s output. For example, adding noise or perturbations to an input text can change the model\\'s understanding and generate incorrect outputs.\\n2. **Prompt Engineering**: Manipulating the prompts given to LLMs can lead to altered responses that may not be in line with the actual desired result. This can include misleading or ambiguous instructions, leading to erroneous conclusions.\\n3. **Adversarial Dialogue Generation**: In conversational scenarios, adversarial dialogue generation can lead to miscommunication or even manipulation of the LLM\\'s responses. These attacks can create a back-and-forth conversation that leads the model astray.\\n4. **Model Poisoning**: This attack involves injecting malicious data into the training dataset. The resulting model may exhibit biased or incorrect behavior on certain inputs, leading to potential security and privacy concerns.\\n5. **Adversarial Embeddings**: In vector space models, adversarially crafted embeddings can manipulate the similarity between different entities, causing the LLM to misunderstand relationships and contexts.\\n\\nMitigation Strategies for Adversarial Attacks on LLMs:\\n\\n1. **Adversarial Training**: By training LLMs with adversarial examples, we can make them more robust against these attacks. This involves generating adversarial examples during the training process and using them to improve model performance and robustness.\\n2. **Input Validation**: Implementing input validation techniques like checking for grammar, context, and semantic correctness can help prevent some adversarial attacks.\\n3. **Prompt Sanitization**: Preprocessing prompts and removing irrelevant or potentially misleading information can help ensure that the model receives clear and accurate instructions.\\n4. **Model Monitoring**: Continuous monitoring of LLM performance and behavior can help detect anomalous behaviors, such as unexpected changes in output or unusual user interactions.\\n5. **Human-in-the-Loop**: Incorporating human oversight into the decision-making process can add an extra layer of security against adversarial attacks. This could involve manual review of critical outputs or a feedback loop for users to report and correct any misbehavior.\\n6. **Adversarial Detection Techniques**: Utilizing various machine learning techniques, such as anomaly detection algorithms and classification models, can help identify and mitigate potential adversarial attacks on LLMs.\\n7. **Secure Training Datasets**: Ensuring the security of training datasets and maintaining their privacy is crucial for preventing model poisoning attacks. This could involve using encryption techniques or access control mechanisms to protect sensitive data.\\n8. **Community Involvement**: Encouraging a responsible AI community that actively engages in research and development of mitigation strategies can help improve overall security against adversarial attacks on LLMs.\\n\\nAdversarial attacks pose significant challenges for large language models, but with the right strategies and techniques, we can enhance their robustness and reliability. By staying informed about the latest developments in this field and collaborating with experts, we can work towards building more secure and trustworthy AI systems.\\n\\nnlp language-model adversarial-attacks security privacy mitigation-strategies prompt-engineering model-poisoning robustness input-validation human-in-the-loop machine-learning anomaly-detection encryption access-control responsible-ai collaboration research-development security-challenges\\n\\n\"\\n\\nIn recent years, Large Language Models (LLMs) have shown impressive results in various natural language processing tasks. However, their power comes with challenges, including understanding their behavior and ensuring their security against potential adversarial attacks. In this post, we will discuss some of the key aspects of using LLMs for problem solving and autonomous agents.\\n\\nProblem Solving with Large Language Models:\\n\\n1. **Prompt Engineering**: Utilizing effective prompts to guide LLMs in solving complex problems is crucial. This involves crafting clear and concise instructions, as well as considering the context of the problem at hand.\\n2. **In-context Reasoning**: By providing examples and context for LLMs to reason from, we can help them solve more intricate tasks and understand relationships between different concepts.\\n3. **External Knowledge Integration**: Incorporating external knowledge sources into the problem solving process can expand the capabilities of LLMs, making them more versatile and adaptable to various scenarios.\\n4. **Reasoning and Planning**: By combining LLMs with reasoning and planning algorithms like MRKL or In-context RL, we can create autonomous agents that can learn from their environment and make informed decisions.\\n5. **Steerability**: Providing a method for controlling the agent\\'s actions, either through an API or command line interface, enables users to interact with LLMs in a more effective way, making them more useful in various applications.\\n\\nAutonomous Agents using Large Language Models:\\n\\n1. **User Interaction**: Designing intuitive user interfaces for autonomous agents can improve the user experience and ensure that they are being used effectively. This could include voice or text-based interfaces, as well as visualizations to help users better understand the agent\\'s behavior.\\n2. **Versatility and Adaptability**: By equipping LLMs with the ability to learn from their environment, we can create autonomous agents that can adapt to various scenarios and solve problems in real time.\\n3. **Integration with External Systems**: Incorporating LLMs into larger systems and applications allows for more advanced problem solving capabilities and improved overall system performance.\\n4. **Steerability and Control**: Implementing methods for users to interactively control the agent\\'s behavior, such as setting goals or adjusting parameters, can make them more valuable in real-world scenarios.\\n\\nAdversarial Attacks on Large Language Models:\\n\\n1. **Input Adversarial Attacks**: These attacks focus on crafting malicious inputs to manipulate the model\\'s output. For example, adding noise or perturbations to an input text can change the model\\'s understanding and generate incorrect outputs.\\n2. **Prompt Engineering**: Manipulating the prompts given to LLMs can lead to altered responses that may not be in line with the actual desired result. This can include misleading or ambiguous instructions, leading to erroneous conclusions.\\n3. **Adversarial Dialogue Generation**: In conversational scenarios, adversarial dialogue generation can lead to miscommunication or even manipulation of the LLM\\'s responses. These attacks can create a back-and-forth conversation that leads the model astray.\\n4. **Model Poisoning**: This attack involves injecting malicious data into the training dataset. The resulting model may exhibit biased or incorrect behavior on certain inputs, leading to potential security and privacy concerns.\\n5. **Adversarial Embeddings**: In vector space models, adversarially crafted embeddings can manipulate the similarity between different entities, causing the LLM to misunderstand relationships and contexts.\\n\\nMitigation Strategies for Adversarial Attacks on Large Language Models:\\n\\n1. **Adversarial Training**: By training LLMs with adversarial examples, we can make them more robust against these attacks. This involves generating adversarial examples during the training process and using them to improve model performance and robustness.\\n2. **Input Validation**: Implementing input validation techniques like checking for grammar, context, and semantic correctness can help prevent some adversarial attacks.\\n3. **Prompt Sanitization**: Preprocessing prompts and removing irrelevant or potentially misleading information can help ensure that the model receives clear and accurate instructions.\\n4. **Model Monitoring**: Continuous monitoring of LLM performance and behavior can help detect anomalous behaviors, such as unexpected changes in output or unusual user interactions.\\n5. **Human-in-the-Loop**: Incorporating human oversight into the problem solving process can help mitigate potential negative consequences of adversarial attacks on LLMs.\\n\\nBy understanding both the opportunities and challenges presented by Large Language Models, we can effectively harness their power to solve complex problems and create autonomous agents that can adapt to real-world environments while remaining secure from adversarial attacks.\\n\\nproblem solving large language models, autonomous agents, prompt engineering, in-context reasoning, external knowledge integration, reasoning and planning, user interaction, versatility and adaptability, integration with external systems, steerability, input adversarial attacks, prompt engineering attacks, adversarial dialogue generation, model poisoning, adversarial embeddings, adversarial training, input validation, prompt sanitization, model monitoring, human-in-the-loop\\n\\nIn today\\'s world, large language models have become a powerful tool for solving complex problems and creating autonomous agents. They can be used to understand natural language, generate text, and even reason about the world around us. However, with great power comes great responsibility. In this post, we will discuss some of the challenges that come with using large language models, specifically in terms of adversarial attacks.\\n\\nAdversarial Attacks on Large Language Models:\\n\\n1. **Input Adversarial Attacks**: These attacks focus on crafting malicious inputs to manipulate the model\\'s output. For example, adding noise or perturbations to an input text can change the model\\'s understanding and generate incorrect outputs. This could lead to misinformation being spread or incorrect conclusions being drawn from data.\\n2. **Prompt Engineering Attacks**: Manipulating the prompts given to large language models can lead to altered responses that may not be in line with the actual desired result. Misleading or ambiguous instructions can cause the model to generate erroneous conclusions, potentially leading to negative consequences.\\n3. **Adversarial Dialogue Generation**: In conversational scenarios, adversarial dialogue generation attacks can lead to miscommunication or even manipulation of the large language model\\'s responses. These attacks can create a back-and-forth conversation that leads the model astray and results in incorrect information being generated.\\n4. **Model Poisoning**: This attack involves injecting malicious data into the training dataset. The resulting model may exhibit biased or incorrect behavior on certain inputs, leading to potential security and privacy concerns.\\n5. **Adversarial Embeddings**: In vector space models, adversarially crafted embeddings can manipulate the similarity between different entities, causing the large language model to misunderstand relationships and contexts. This could lead to incorrect conclusions being drawn or misinformation being spread.\\n\\nMitigation Strategies for Adversarial Attacks on Large Language Models:\\n\\n1. **Adversarial Training**: By training large language models with adversarial examples, we can make them more robust against these attacks. This involves generating adversarial examples during the training process and using them to improve model performance and robustness.\\n2. **Input Validation**: Implementing input validation techniques like checking for grammar, context, and semantic correctness can help prevent some adversarial attacks. Additionally, using data from trusted sources and validating it before feeding it into the model can reduce the likelihood of model poisoning attacks.\\n3. **Prompt Sanitization**: Preprocessing prompts and removing irrelevant or potentially misleading information can help ensure that the large language model receives clear and accurate instructions. This could involve using pre-defined templates for certain tasks or implementing rules to remove ambiguous or misleading phrases from user queries.\\n4. **Model Monitoring**: Continuous monitoring of large language models\\' performance and behavior can help detect anomalous behaviors, such as unexpected changes in output or unusual user interactions. This could include implementing alerts for potential attacks and regularly auditing the model\\'s outputs to ensure they are accurate and unbiased.\\n5. **Human-in-the-Loop**: Incorporating human oversight into the problem solving process can help mitigate potential negative consequences of adversarial attacks on large language models. This could involve having a team of experts review the model\\'s outputs for accuracy and validity, as well as implementing feedback loops to improve the model\\'s performance over time.\\n\\nIn conclusion, using large language models comes with its own set of challenges, specifically in terms of adversarial attacks. By understanding these attack types and implementing appropriate mitigation strategies like adversarial training, input validation, prompt sanitization, model monitoring, and human-in-the-loop, we can build more robust and secure large language models that can effectively solve complex problems and create autonomous agents while minimizing potential negative consequences.\\n\\nadversarial attacks, large language models, input adversarial attacks, prompt engineering attacks, adversarial dialogue generation, model poisoning, adversarial embeddings, adversarial training, input validation, prompt sanitization, model monitoring, human-in-the-loop'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"mistral\", max_tokens=100) # temperature=0.8\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"refine\")\n",
    "\n",
    "chain.run(docs)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
